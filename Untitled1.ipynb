{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ba3e17-e24a-4d61-9334-c4f4de3a5a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing to fetch 1000 movies\n",
      "Saving to: C:\\Users\\Notandi\\Desktop\\Social graph\\Lokaverkefni\\data\\imdb_txt_fast\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "FAST MODE: MovieLens -> IMDb /reference only -> One TXT per movie (Title_YYYY.txt)\n",
    "\n",
    "Speed-ups:\n",
    "  • 1 HTTP request per movie (reference page only)\n",
    "  • requests.Session() with connection pooling\n",
    "  • ThreadPoolExecutor concurrency (tune MAX_WORKERS)\n",
    "  • Resume-safe: skip files that already exist\n",
    "\n",
    "Outputs:\n",
    "  • data/imdb_txt_fast/Title_YYYY.txt (movieId, imdb_id, title, year, url, ratingValue, ratingCount, genres, directors, actors, duration, description)\n",
    "  • data/imdb_txt_fast/index_imdb_txt.csv (movieId, tt, title, year, txt_filename, imdb_url)\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import time\n",
    "import html\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "# ---------------- Config ----------------\n",
    "LINKS_CSV   = \"Movielens/links.csv\"                # MovieLens\n",
    "MOVIES_CSV  = \"Movielens/movies.csv\"               # for fallback display title if needed\n",
    "OUT_DIR     = Path(\"data/imdb_txt_fast\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SELECT_BY   = \"first_n\"                  # \"first_n\" or \"all\"\n",
    "FIRST_N     = 1000\n",
    "\n",
    "TOP_ACTORS_PER_MOVIE = 20                # cap actors saved per movie\n",
    "MAX_WORKERS = 8                          # concurrency (be polite; 6–10 is reasonable)\n",
    "REQUEST_TIMEOUT = 20\n",
    "RETRIES = 3\n",
    "\n",
    "UA = {\n",
    "    \"User-Agent\": (\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "        \"Chrome/123.0 Safari/537.36\"\n",
    "    ),\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "}\n",
    "\n",
    "# -------------- Helpers -----------------\n",
    "def imdb_tt_from_numeric(imdb_numeric: int) -> str:\n",
    "    return f\"tt0{int(imdb_numeric):06d}\"\n",
    "\n",
    "def safe_title_for_filename(title: str) -> str:\n",
    "    s = (title or \"\").strip()\n",
    "    s = s.replace(\" \", \"_\")\n",
    "    s = re.sub(r'[\\\\/:*?\"<>|]+', \"_\", s)\n",
    "    s = re.sub(r\"_+\", \"_\", s)\n",
    "    return s or \"untitled\"\n",
    "\n",
    "def human_duration_from_reference(runtime_text: str) -> str:\n",
    "    \"\"\"\n",
    "    On /reference, runtime often looks like '142 min' or '1 hr 36 min'.\n",
    "    Convert to '2h 22m' style when possible.\n",
    "    \"\"\"\n",
    "    txt = (runtime_text or \"\").lower()\n",
    "    # patterns: \"1 hr 36 min\", \"142 min\"\n",
    "    m = re.search(r'(\\d+)\\s*hr', txt)\n",
    "    h = int(m.group(1)) if m else None\n",
    "    m = re.search(r'(\\d+)\\s*min', txt)\n",
    "    mins = int(m.group(1)) if m else None\n",
    "    if h is not None and mins is not None:\n",
    "        return f\"{h}h {mins}m\"\n",
    "    if mins is not None:\n",
    "        return f\"{mins}m\"\n",
    "    return runtime_text.strip()\n",
    "\n",
    "def build_session():\n",
    "    s = requests.Session()\n",
    "    retries = Retry(\n",
    "        total=RETRIES,\n",
    "        backoff_factor=0.5,\n",
    "        status_forcelist=[429, 500, 502, 503, 504],\n",
    "        allowed_methods=[\"GET\"]\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retries, pool_connections=MAX_WORKERS, pool_maxsize=MAX_WORKERS)\n",
    "    s.mount(\"https://\", adapter); s.mount(\"http://\", adapter)\n",
    "    s.headers.update(UA)\n",
    "    return s\n",
    "\n",
    "# ----------- Parsing /reference (single-source) -----------\n",
    "def parse_reference(html_text: str):\n",
    "    \"\"\"\n",
    "    Extract main fields from /reference page:\n",
    "    returns dict: name, year, ratingValue, ratingCount, genres(list), directors(list), actors(list), duration(str), description(str)\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html_text, \"html.parser\")\n",
    "    out = {\n",
    "        \"name\": None, \"year\": None, \"ratingValue\": None, \"ratingCount\": None,\n",
    "        \"genres\": [], \"directors\": [], \"actors\": [], \"duration\": \"\", \"description\": \"\"\n",
    "    }\n",
    "\n",
    "    # Title & year from <title> or h3\n",
    "    ttag = soup.find(\"title\")\n",
    "    if ttag and ttag.text:\n",
    "        ttxt = ttag.text.replace(\" - IMDb\", \"\").strip()\n",
    "        out[\"name\"] = re.sub(r\"\\s*\\(\\d{4}\\)$\", \"\", ttxt).strip()\n",
    "        m = re.search(r\"\\((\\d{4})\\)$\", ttxt)\n",
    "        if m:\n",
    "            out[\"year\"] = m.group(1)\n",
    "\n",
    "    # Rating value / count\n",
    "    # Look for itemprop or known labels\n",
    "    rv = soup.find(attrs={\"itemprop\": \"ratingValue\"})\n",
    "    if rv and rv.text.strip():\n",
    "        out[\"ratingValue\"] = rv.text.strip()\n",
    "    rc = soup.find(attrs={\"itemprop\": \"ratingCount\"})\n",
    "    if rc and rc.text.strip():\n",
    "        out[\"ratingCount\"] = re.sub(r\"[^\\d]\", \"\", rc.text)\n",
    "\n",
    "    # Sections are often under h5/h4 labels on /reference\n",
    "    # We'll scan labeled blocks nearby.\n",
    "    def collect_names_after_header(header_substrs, limit=None):\n",
    "        names = []\n",
    "        for hdr in soup.find_all([\"h2\",\"h3\",\"h4\",\"h5\"]):\n",
    "            label = hdr.get_text(\" \", strip=True).lower()\n",
    "            if any(sub in label for sub in header_substrs):\n",
    "                for a in hdr.find_all_next(\"a\", href=True):\n",
    "                    href = a[\"href\"]\n",
    "                    if \"/name/nm\" in href:\n",
    "                        nm = a.get_text(strip=True)\n",
    "                        if nm and nm not in names:\n",
    "                            names.append(nm)\n",
    "                        if limit and len(names) >= limit:\n",
    "                            return names\n",
    "                    # stop if we hit another header\n",
    "                    if a.find_previous([\"h2\",\"h3\",\"h4\",\"h5\"]) is not hdr:\n",
    "                        break\n",
    "                break\n",
    "        return names\n",
    "\n",
    "    # Directors\n",
    "    out[\"directors\"] = collect_names_after_header([\"directed by\", \"director\"], limit=None)\n",
    "\n",
    "    # Actors (Cast) — limit to TOP_ACTORS_PER_MOVIE\n",
    "    actors = collect_names_after_header([\"cast\"], limit=TOP_ACTORS_PER_MOVIE)\n",
    "    out[\"actors\"] = actors\n",
    "\n",
    "    # Genres\n",
    "    # Find a 'Genres' header and gather following text/links until next header\n",
    "    genres = []\n",
    "    for hdr in soup.find_all([\"h2\",\"h3\",\"h4\",\"h5\"]):\n",
    "        label = hdr.get_text(\" \", strip=True).lower()\n",
    "        if \"genres\" in label or \"genre\" in label:\n",
    "            # collect nearby links or list items\n",
    "            for a in hdr.find_all_next(\"a\", href=True):\n",
    "                txt = a.get_text(strip=True)\n",
    "                if txt and len(txt) < 40 and \"/search/title/?genres=\" in a[\"href\"].lower():\n",
    "                    if txt not in genres:\n",
    "                        genres.append(txt)\n",
    "                if a.find_previous([\"h2\",\"h3\",\"h4\",\"h5\"]) is not hdr:\n",
    "                    break\n",
    "            break\n",
    "    out[\"genres\"] = genres\n",
    "\n",
    "    # Runtime\n",
    "    runtime = \"\"\n",
    "    for hdr in soup.find_all([\"h2\",\"h3\",\"h4\",\"h5\"]):\n",
    "        lbl = hdr.get_text(\" \", strip=True).lower()\n",
    "        if \"runtime\" in lbl:\n",
    "            # collect the first text after header\n",
    "            nxt = hdr.find_next()\n",
    "            # search forward for a tag that has minutes text\n",
    "            limit_nodes = 40\n",
    "            while nxt and limit_nodes > 0:\n",
    "                txt = nxt.get_text(\" \", strip=True) if hasattr(nxt, \"get_text\") else \"\"\n",
    "                if re.search(r\"\\b\\d+\\s*min\\b\", txt) or re.search(r\"\\b\\d+\\s*hr\", txt):\n",
    "                    runtime = txt\n",
    "                    break\n",
    "                if nxt.name in (\"h2\",\"h3\",\"h4\",\"h5\"):\n",
    "                    break\n",
    "                nxt = nxt.find_next()\n",
    "                limit_nodes -= 1\n",
    "            break\n",
    "    out[\"duration\"] = human_duration_from_reference(runtime)\n",
    "\n",
    "    # Description / Plot\n",
    "    # Look for \"Plot\" or \"Storyline\" or \"Summary\"\n",
    "    desc = \"\"\n",
    "    for hdr in soup.find_all([\"h2\",\"h3\",\"h4\",\"h5\"]):\n",
    "        lbl = hdr.get_text(\" \", strip=True).lower()\n",
    "        if any(k in lbl for k in [\"plot\", \"storyline\", \"summary\"]):\n",
    "            # take a paragraph after this header\n",
    "            p = hdr.find_next(\"p\")\n",
    "            if p and p.get_text(strip=True):\n",
    "                desc = p.get_text(\" \", strip=True)\n",
    "                break\n",
    "    out[\"description\"] = html.unescape(desc)\n",
    "\n",
    "    return out\n",
    "\n",
    "# -------------- Load data & select set --------------\n",
    "links = pd.read_csv(LINKS_CSV).dropna(subset=[\"imdbId\"])\n",
    "links[\"imdbId\"] = links[\"imdbId\"].astype(int)\n",
    "links[\"tt\"] = links[\"imdbId\"].apply(imdb_tt_from_numeric)\n",
    "\n",
    "# Fallback display title from MovieLens (for year/title if parsing fails)\n",
    "try:\n",
    "    movies = pd.read_csv(MOVIES_CSV)[[\"movieId\", \"title\"]]\n",
    "except Exception:\n",
    "    movies = pd.DataFrame(columns=[\"movieId\", \"title\"])\n",
    "\n",
    "df = links.merge(movies, on=\"movieId\", how=\"left\")\n",
    "sel = df.head(FIRST_N) if SELECT_BY == \"first_n\" else df\n",
    "\n",
    "print(f\"Preparing to fetch {len(sel)} movies\")\n",
    "print(\"Saving to:\", OUT_DIR.resolve())\n",
    "\n",
    "# Resume-safe: skip existing files\n",
    "existing = {p.name for p in OUT_DIR.glob(\"*.txt\")}\n",
    "\n",
    "index_rows = []\n",
    "\n",
    "# ----------- Worker -----------\n",
    "def fetch_one(session: requests.Session, row):\n",
    "    movieId = int(row.movieId)\n",
    "    tt = row.tt\n",
    "    url = f\"https://www.imdb.com/title/{tt}/reference\"\n",
    "    try:\n",
    "        r = session.get(url, timeout=REQUEST_TIMEOUT)\n",
    "        r.raise_for_status()\n",
    "        data = parse_reference(r.text)\n",
    "\n",
    "        # fallback title/year from MovieLens title \"(YYYY)\"\n",
    "        ml_title = row.title if isinstance(row.title, str) else \"\"\n",
    "        if not data[\"name\"] and ml_title:\n",
    "            data[\"name\"] = re.sub(r\"\\s*\\(\\d{4}\\)\\s*$\", \"\", ml_title).strip()\n",
    "        if not data[\"year\"] and ml_title:\n",
    "            m = re.search(r\"\\((\\d{4})\\)\", ml_title)\n",
    "            if m: data[\"year\"] = m.group(1)\n",
    "\n",
    "        title = data[\"name\"] or tt\n",
    "        year = data[\"year\"] or \"\"\n",
    "        fname = f\"{safe_title_for_filename(title)}_{year}.txt\" if year else f\"{safe_title_for_filename(title)}.txt\"\n",
    "        out_path = OUT_DIR / fname\n",
    "\n",
    "        if out_path.name in existing:\n",
    "            # already written in this run or previous run\n",
    "            index_rows.append({\n",
    "                \"movieId\": movieId, \"tt\": tt, \"title\": title, \"year\": year,\n",
    "                \"txt_filename\": out_path.name, \"imdb_url\": f\"https://www.imdb.com/title/{tt}/\"\n",
    "            })\n",
    "            return (\"SKIP\", tt, out_path.name)\n",
    "\n",
    "        # Format fields\n",
    "        genres = \", \".join(data[\"genres\"])\n",
    "        directors = \", \".join(data[\"directors\"])\n",
    "        actors = \", \".join(data[\"actors\"][:TOP_ACTORS_PER_MOVIE])\n",
    "        duration = data[\"duration\"]\n",
    "        ratingValue = data[\"ratingValue\"] or \"\"\n",
    "        ratingCount = data[\"ratingCount\"] or \"\"\n",
    "        description = data[\"description\"]\n",
    "\n",
    "        # Write TXT\n",
    "        lines = []\n",
    "        lines.append(f\"movieId: {movieId}\")\n",
    "        lines.append(f\"imdb_id: {tt}\")\n",
    "        lines.append(f\"title: {title}\")\n",
    "        lines.append(f\"year: {year}\")\n",
    "        lines.append(f\"url: https://www.imdb.com/title/{tt}/\")\n",
    "        lines.append(f\"ratingValue: {ratingValue}\")\n",
    "        lines.append(f\"ratingCount: {ratingCount}\")\n",
    "        lines.append(f\"genres: {genres}\")\n",
    "        lines.append(f\"directors: {directors}\")\n",
    "        lines.append(f\"actors: {actors}\")\n",
    "        lines.append(f\"duration: {duration}\")\n",
    "        lines.append(\"\")\n",
    "        lines.append(\"description:\")\n",
    "        lines.append(description)\n",
    "        out_path.write_text(\"\\n\".join(lines), encoding=\"utf-8\")\n",
    "\n",
    "        existing.add(out_path.name)\n",
    "        index_rows.append({\n",
    "            \"movieId\": movieId, \"tt\": tt, \"title\": title, \"year\": year,\n",
    "            \"txt_filename\": out_path.name, \"imdb_url\": f\"https://www.imdb.com/title/{tt}/\"\n",
    "        })\n",
    "        return (\"OK\", tt, out_path.name)\n",
    "    except Exception as e:\n",
    "        return (\"FAIL\", tt, str(e))\n",
    "\n",
    "# ----------- Run concurrent fetch -----------\n",
    "session = build_session()\n",
    "futures = []\n",
    "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:\n",
    "    for row in sel.itertuples(index=False):\n",
    "        futures.append(ex.submit(fetch_one, session, row))\n",
    "\n",
    "done = ok = skip = fail = 0\n",
    "for fut in as_completed(futures):\n",
    "    status, tt, info = fut.result()\n",
    "    done += 1\n",
    "    if status == \"OK\":\n",
    "        ok += 1; print(f\"[{done:>5}/{len(futures)}] OK   {tt} -> {info}\")\n",
    "    elif status == \"SKIP\":\n",
    "        skip += 1; print(f\"[{done:>5}/{len(futures)}] SKIP {tt} ({info})\")\n",
    "    else:\n",
    "        fail += 1; print(f\"[{done:>5}/{len(futures)}] FAIL {tt}: {info}\")\n",
    "\n",
    "# Write index CSV\n",
    "pd.DataFrame(index_rows).to_csv(OUT_DIR / \"index_imdb_txt.csv\", index=False)\n",
    "print(\"\\nDone.\")\n",
    "print(f\"OK: {ok} | SKIP: {skip} | FAIL: {fail}\")\n",
    "print(\"Folder:\", OUT_DIR.resolve())\n",
    "print(\"Index:\", OUT_DIR / \"index_imdb_txt.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c97685-95da-4bee-995a-d708df404654",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
